{
    "product_id": "api_e2f01ed2-14b1-4db1-baab-d585eb289614",
    "tool_description": "Splits text into sensible semantic chunks, grouping together sentences on the same topic. These chunks of text based on meaning are useful for enhanced embedding/retrieval, for downstream processing of text, and displaying unstructured text to the user in a useful, readable way. Perfect for OCR postprocessing.",
    "home_url": "https://rapidapi.com/rapidinterconnect/api/text-semantic-split-into-chunks/",
    "name": "Text Semantic Split - into Chunks",
    "title": "Text Semantic Split - into Chunks",
    "pricing": "FREEMIUM",
    "tool_name": "Text Semantic Split - into Chunks",
    "score": {
        "avgServiceLevel": 0,
        "avgLatency": 127234,
        "avgSuccessRate": 0,
        "popularityScore": 0,
        "__typename": "Score"
    },
    "host": "text-semantic-split-into-chunks.p.rapidapi.com",
    "api_list": [
        {
            "name": "Semantically Segment Text",
            "url": "https://text-semantic-split-into-chunks.p.rapidapi.com/subpart/langsam/autochapter",
            "description": "Pass your text to receive it back semantically split into a set of lists corresponding to semantic chunks. See the examples for behaviour.\n\nYou can use the resulting sentences to improve semantic search. Because the chunks are optimised to be similar length to search result snippets, they work great with MS MARCO or hypothetical embeddings (and GPT/OpenAI embeddings), as they capture multi-sentence ideas while not exceeding the point at which embeddings cannot contain information on all parts of the chunk. We have observed massive performance boosts and positive user feedback when they are returned chunk-based search results rather than sentences or normal paragraph based split content.\n\nA second use case we use it for is to process related material together, by providing it as context in prompts to large language models like GPT-3.\n\nYou can ask for example, for a summary of each semantic chunk and pass in the chunk. This will be much more useful than arbitrarily summarising every 5 sentences.",
            "method": "POST",
            "required_parameters": [],
            "optional_parameters": [],
            "code": "import requests\n\nurl = \"https://text-semantic-split-into-chunks.p.rapidapi.com/subpart/langsam/autochapter\"\n\nheaders = {\n            \"X-RapidAPI-Key\": \"SIGN-UP-FOR-KEY\",\n            \"X-RapidAPI-Host\": \"text-semantic-split-into-chunks.p.rapidapi.com\"\n        }\n\nresponse = requests.post(url, headers=headers)\nprint(response.json())\n",
            "convert_code": "import requests\n\nurl = \"https://text-semantic-split-into-chunks.p.rapidapi.com/subpart/langsam/autochapter\"\n\nheaders = {\n            \"X-RapidAPI-Key\": \"SIGN-UP-FOR-KEY\",\n            \"X-RapidAPI-Host\": \"text-semantic-split-into-chunks.p.rapidapi.com\"\n        }\n\nresponse = requests.post(url, headers=headers)\nprint(response.json())\n",
            "test_endpoint": "",
            "statuscode": 200,
            "schema": {
                "type": "object",
                "properties": {
                    "get": {
                        "type": "string"
                    },
                    "parameters": {
                        "type": "array"
                    },
                    "errors": {
                        "type": "array"
                    },
                    "results": {
                        "type": "integer"
                    },
                    "response": {
                        "type": "array",
                        "items": {
                            "type": "object"
                        }
                    }
                }
            }
        }
    ]
}